[
  {
    "objectID": "apps/llm_writes_gtfs_explorer.html",
    "href": "apps/llm_writes_gtfs_explorer.html",
    "title": "If you prompt an LLM to Shine on the GTFS‚Ä¶",
    "section": "",
    "text": "I‚Äôm testing out how LLM chats can improve my workflow.\nThe other day (from several weeks ago) I went over to Claude and asked them to write a Shiny App that allows the user to upload a GTFS zip file and explore the transit system described within the upload. This is the result. I haven‚Äôt edited it really, just wanted to see what kind of output we get here. I think it‚Äôs pretty neat.\nI moved to Washington, D.C. over three years ago to accept a inaugural position of Transit & Rail Data Scientist at an engineering and public transportation consultancy firm. The General Transit Feed Specification (GTFS) featured prominently in my day-to-day work; it is a standardized mapping (through data) of the operations of a transit agency. You can find further documentation of the GTFS here.\nThis app offers the user the chance to upload a GTFS zip file. They used to be easier to obtain and download from reputable sources, but now you must create accounts with these websites and thus jump through some hoops. I found this link that allows us to download the Metrorail GTFS fof the Washington Metropolitan Area Transit Authority (WMATA). Make sure you click on the link that says ‚ÄúLatest GTFS Feed - Corrected & Enriched‚Äù.\nOnce you‚Äôve downloaded the zip file, proceed with uploaded that zip file into this app, and give it a shot at trying to understand what goes on inside the GTFS :)"
  },
  {
    "objectID": "apps/mta_art_explorer.html",
    "href": "apps/mta_art_explorer.html",
    "title": "MTA Art Explorer",
    "section": "",
    "text": "Explore the permanent art collection in MTA stations across New York City. This is intended to demonstrate how quickly one can spin up an app that enables quick and easy exploration of a data set from New York State‚Äôs Open Data Portal. The code will be published to GitHub shortly.\nTo identify the cover photo, it is the piece The Habitat for the Yellow Bird by Takayo Noda, and is displayed in the Sutter Ave station on the L line of the NYC Subway.\n\n\nThe data for this app can be found in the MTA‚Äôs Open Data Portal."
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Plot with Me",
    "section": "",
    "text": "Browse through the data sets I‚Äôve interactively visualized with the help of Shiny.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIf you prompt an LLM to Shine on the GTFS‚Ä¶\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMTA Art Explorer\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "demos/BigMacIndex.html",
    "href": "demos/BigMacIndex.html",
    "title": "25 Years of the Big Mac Index",
    "section": "",
    "text": "Author note: I‚Äôm letting ClaudeAI take the wheel here (maybe because Anthropic has partnered with The Economist and this‚Äôll be cute for them)."
  },
  {
    "objectID": "demos/BigMacIndex.html#the-big-mac-index-economics-made-delicious",
    "href": "demos/BigMacIndex.html#the-big-mac-index-economics-made-delicious",
    "title": "25 Years of the Big Mac Index",
    "section": "The Big Mac Index: Economics Made Delicious",
    "text": "The Big Mac Index: Economics Made Delicious\nü§ñ Section written by Claude\nIn September 1986, The Economist journalist Pam Woodall introduced the Big Mac Index as a semi-humorous illustration of Purchasing Power Parity (PPP). The theory behind it‚ÄîPPP‚Äîsuggests that exchange rates should adjust so that identical goods cost the same in different countries. Since McDonald‚Äôs Big Macs are sold in nearly identical form across 50+ countries, they make for a surprisingly useful economic indicator.\n\nWhy a Big Mac?\nMcDonald‚Äôs Big Mac was chosen because of the prevalence of the fast food chain worldwide, and because the sandwich remains largely the same across all countries. The Big Mac Index works because:\n\nStandardization: The recipe is consistent globally\nLocal Production: Big Macs contain locally-sourced ingredients and the price reflects many local economic factors, such as ingredient prices, local wages, and advertising costs\nUbiquity: McDonald‚Äôs operates in most major economies\nSimplicity: Easy to understand and compare‚Äîthe index gave rise to the term ‚Äúburgernomics‚Äù\n\nWhile economists widely cite the Big Mac index as a reasonable real-world measurement of purchasing power parity, it was not intended to be a legitimate tool for exchange rate evaluation, though it is now globally recognized and featured in many academic textbooks and reports. The Economist has published this index regularly since 1986, creating a rich dataset for analysis."
  },
  {
    "objectID": "demos/BigMacIndex.html#data-source",
    "href": "demos/BigMacIndex.html#data-source",
    "title": "25 Years of the Big Mac Index",
    "section": "Data Source",
    "text": "Data Source\nü§ñ Section written by Claude\nThe data comes from The Economist‚Äôs GitHub repository, where they maintain historical Big Mac prices and calculated currency valuations. The dataset includes:\n\nRaw prices: Local currency and USD-converted prices\nFull index: Includes over/under-valuation calculations against the US dollar\nTime span: From April 2000 to January 2025 in this analysis"
  },
  {
    "objectID": "demos/BigMacIndex.html#original-approach-unedited-code-with-commentary",
    "href": "demos/BigMacIndex.html#original-approach-unedited-code-with-commentary",
    "title": "25 Years of the Big Mac Index",
    "section": "Original Approach: Unedited Code with Commentary",
    "text": "Original Approach: Unedited Code with Commentary\nBelow is the original code exactly as written, with my commentary on the thought process and decisions.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(extrafont)\nlibrary(plotly)\n\nfonts()\n\n\nü§ñ Claude‚Äôs Commentary: This setup is clean and minimal. Loading plotly suggests the author may have experimented with interactive versions (though it‚Äôs not used in the final plot). The fonts() call displays available fonts‚Äîuseful during development but could be removed for production.\n\n\nCode\nfull &lt;- read_csv(\"data/big-mac-full-index.csv\")\nraw &lt;- read_csv(\"data/big-mac-raw-index.csv\")\n\n\nü§ñ Claude‚Äôs Commentary: Both datasets are loaded, though only full is used. This is common during exploratory analysis‚Äîkeeping raw available in case you need to cross-reference or pivot approaches.\n\n\nCode\ndelta &lt;- \n  full |&gt; \n  filter(date %in% as.Date(c('2000-04-01', '2025-01-01'))) |&gt; \n  select(date, name, dollar_price) |&gt; \n  pivot_wider(names_from = date,\n              values_from = dollar_price,\n              names_repair = 'minimal') |&gt; \n  select(name, before = `2000-04-01`, after = `2025-01-01`) |&gt; \n  filter(!is.na(before), !is.na(after)) |&gt; \n  arrange(name) \n\npercent_change &lt;-\n  delta |&gt; \n  mutate(pct_change = round(((after-before)/before), digits = 2),\n         pct_label = scales::percent(pct_change)) |&gt; \n  arrange(desc(pct_change))\n\n\nü§ñ Claude‚Äôs Commentary: This is a smart approach to calculating change over time. The use of pivot_wider() is particularly elegant‚Äîit transforms dates into columns, making it easy to calculate the difference. The names_repair = 'minimal' argument is important because date columns would otherwise be automatically ‚Äúcleaned‚Äù into invalid R names. Creating both numeric (pct_change) and formatted (pct_label) versions shows good data hygiene‚Äîyou keep the raw numbers for potential calculations while having display-ready labels.\nPotential consideration: This approach filters to countries present in both time periods. Countries that started reporting after 2000 or stopped before 2025 are excluded, which is the right call for a ‚Äúchange over 25 years‚Äù metric.\n\n\nCode\ndestaca &lt;- \n  full |&gt; \n  filter(date %in% as.Date('2025-01-01'), !is.na(dollar_price)) |&gt; \n  arrange(date, desc(dollar_price)) %&gt;% \n  .[c(1:5, 50:54), ]\n\nlabels &lt;- \n  destaca |&gt; \n  transmute(label = paste0(name, \", $\", round(dollar_price, digits = 2)))\n\n\nü§ñ Claude‚Äôs Commentary: This is where things get interesting. The use of .[c(1:5, 50:54), ] is base R subsetting‚Äîgrabbing rows 1-5 (most expensive) and 50-54 (least expensive). This works but is brittle: if the dataset has fewer than 54 rows, this breaks. Also, the specific row indices assume a certain ordering.\nDesign insight: The author is making an aesthetic choice here‚Äîexactly 10 countries highlighted, split between extremes. This creates visual balance.\nThe transmute() for labels is efficient‚Äîit‚Äôs like mutate() but keeps only the new column. The formatted labels are ready-to-use for annotation.\n\n\nCode\nfull |&gt; \n  mutate(check = if_else(is.na(dollar_price) & date == as.Date('2025-01-01'), 'na', 'good'),\n         hl = if_else(name %in% destaca$name, \"yes\", \"no\")) |&gt; \n  filter(check == 'good') |&gt; \n  select(date, name, dollar_price, hl) |&gt; \n  ggplot(aes(x = date, y = dollar_price, group = name)) +\n  geom_line(aes(color = hl),\n            show.legend = FALSE) +\n  labs(title = '25 years of the Big Mac Index from *The Economist*',\n       subtitle = 'Most and Least Expensive Countries to by a Big Mac, USD')+\n  scale_x_date(date_labels = '%Y',\n               limits = as.Date(c('1999-06-01', '2034-01-01')),\n               expand = c(0, 0)) +\n  scale_y_continuous(breaks = c(0:8)) +\n  scale_color_manual(values = c(\"#FFCC00\", \"#05007B\", \"black\")) +\n  annotate('text',\n           x = as.Date('2033-04-09'),\n           y = 0.3:8.3, \n           color = \"#FFCC00\",\n           label = paste0(\"$\", 0:8),\n           size = 6,\n           family = 'Cambria') +\n  annotate(\"text\",\n           x = as.Date(\"2025-02-28\"),\n           y = c(8, 7.35, 7, 6.6, 5.95, 3.4, 3.05, 2.7, 2.35, 2),\n           label = labels$label,\n           family = 'Cambria',\n           size = 4.5,\n           color = \"#05007B\",\n           hjust = 0) +\n  theme_minimal() +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(color = \"#FFCC00\", size = 10),\n    panel.grid = element_blank(),\n    text = element_text(family = \"Cambria\", face = \"bold\", color = \"#FFCC00\"),\n    plot.background = element_rect(fill = \"#DA291C\", color = \"#DA291C\"),\n    panel.background = element_rect(fill = \"#DA291C\", color = \"#DA291C\"),\n    axis.title = element_blank(),\n    plot.title = element_markdown(),\n  ) \n\n\n\n\n\nü§ñ Claude‚Äôs Commentary on this approach:\nWhat works really well:\n\nThe filtering logic: The check variable elegantly filters out countries with no 2025 data, ensuring we only plot complete series\nColor hierarchy: Three colors (yellow highlight, dark blue for labels, black for background lines) creates clear visual hierarchy\nManual annotation positioning: Those y-coordinates (c(8, 7.35, 7, 6.6, 5.95...)) were clearly hand-tuned to avoid overlaps‚Äîthis is often the only way to get perfect label placement\nExtended x-axis: The limits = as.Date(c('1999-06-01', '2034-01-01')) is clever‚Äîit extends well into the future to create space for labels on the right\nMcDonald‚Äôs branding: The color scheme (#DA291C red, #FFCC00 yellow, #05007B dark blue) immediately evokes the brand\n\nPotential improvements:\n\nThe y-coordinates for labels are hardcoded: If the data changes or different countries are highlighted, these positions would need manual adjustment\nTypo in subtitle: ‚Äúby a Big Mac‚Äù should be ‚Äúbuy a Big Mac‚Äù\nThe x-position for y-axis labels ('2033-04-09') seems arbitrary‚Äîunclear why this specific date\nNo legend: While show.legend = FALSE cleans up the plot, there‚Äôs no legend explaining what the colored lines mean (though it‚Äôs somewhat obvious from context)\n\nOverall assessment: This is a strong visualization with intentional design choices. The manual positioning shows attention to detail, and the McDonald‚Äôs color scheme is a sophisticated branding choice. The code is readable and the pipeline logic is clear. The brittleness around hardcoded positions is the main weakness‚Äîbut sometimes that‚Äôs necessary for publication-quality plots."
  },
  {
    "objectID": "demos/BigMacIndex.html#alternative-approach-claudes-take",
    "href": "demos/BigMacIndex.html#alternative-approach-claudes-take",
    "title": "25 Years of the Big Mac Index",
    "section": "Alternative Approach: Claude‚Äôs Take",
    "text": "Alternative Approach: Claude‚Äôs Take\nü§ñ This entire section written and coded by Claude\nHere‚Äôs how I would approach the same visualization, with a focus on making the code more robust and self-documenting:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(scales)\n\n# Read data\nfull &lt;- read_csv(\"data/big-mac-full-index.csv\", show_col_types = FALSE)\n\n\n\n\nCode\n# Filter to complete time series (countries with 2025 data)\ncountries_with_2025 &lt;- full |&gt; \n  filter(date == as.Date('2025-01-01'), !is.na(dollar_price)) |&gt; \n  pull(name)\n\nplot_data &lt;- full |&gt; \n  filter(name %in% countries_with_2025,\n         date &gt;= as.Date('2000-01-01')) |&gt; \n  select(date, name, dollar_price)\n\n# Identify extreme countries more robustly\nextremes_2025 &lt;- plot_data |&gt; \n  filter(date == max(date)) |&gt; \n  arrange(desc(dollar_price)) |&gt; \n  slice(c(1:5, (n()-4):n())) |&gt;  # Top 5 and bottom 5\n  mutate(\n    label = paste0(name, \", \", dollar(dollar_price, accuracy = 0.01)),\n    # Calculate label positions with spacing\n    y_pos = dollar_price,\n    x_pos = max(plot_data$date) + days(60)\n  )\n\n# Add highlight flag to plot data\nplot_data &lt;- plot_data |&gt; \n  mutate(\n    highlight = case_when(\n      name %in% extremes_2025$name ~ \"labeled\",\n      TRUE ~ \"background\"\n    )\n  )\n\n\nü§ñ My reasoning:\n\nRather than hardcoding row indices, I use slice(c(1:5, (n()-4):n())) which adapts to any dataset size\nI calculate label positions relative to the data (max(date) + days(60)) rather than hardcoding dates\nI use dollar() from scales for consistent currency formatting\nI create the highlight flag more explicitly with case_when()\n\n\n\nCode\n# Define McDonald's colors\nmc_red &lt;- \"#DA291C\"\nmc_yellow &lt;- \"#FFCC00\"\nmc_blue &lt;- \"#27251F\"  # Using a dark neutral instead of dark blue\n\nggplot(plot_data, aes(x = date, y = dollar_price, group = name)) +\n  # Background lines for all countries\n  geom_line(\n    data = filter(plot_data, highlight == \"background\"),\n    color = alpha(\"black\", 0.3),\n    linewidth = 0.3\n  ) +\n  # Highlighted lines for labeled countries\n  geom_line(\n    data = filter(plot_data, highlight == \"labeled\"),\n    color = mc_yellow,\n    linewidth = 0.8\n  ) +\n  # Country labels\n  geom_text(\n    data = extremes_2025,\n    aes(x = x_pos, y = y_pos, label = label),\n    hjust = 0,\n    size = 4,\n    color = mc_yellow,\n    fontface = \"bold\",\n    inherit.aes = FALSE\n  ) +\n  # Y-axis price labels on the right\n  scale_y_continuous(\n    breaks = 0:8,\n    limits = c(0, 8.5),\n    sec.axis = sec_axis(\n      ~ .,\n      breaks = 0:8,\n      labels = dollar_format()(0:8)\n    )\n  ) +\n  scale_x_date(\n    date_labels = \"%Y\",\n    expand = expansion(mult = c(0.02, 0.25))  # More room on right for labels\n  ) +\n  labs(\n    title = \"25 years of the Big Mac Index from *The Economist*\",\n    subtitle = \"Most and least expensive countries to **buy** a Big Mac, USD\",\n    caption = \"Data: The Economist Big Mac Index | Visualization: Claude\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    # McDonald's brand colors\n    plot.background = element_rect(fill = mc_red, color = NA),\n    panel.background = element_rect(fill = mc_red, color = NA),\n    \n    # Text styling\n    text = element_text(color = mc_yellow, face = \"bold\"),\n    plot.title = element_markdown(size = 16, margin = margin(b = 5)),\n    plot.subtitle = element_markdown(size = 11, margin = margin(b = 15)),\n    plot.caption = element_text(size = 8, hjust = 0),\n    \n    # Axes\n    axis.text = element_text(color = mc_yellow),\n    axis.text.y.right = element_text(color = mc_yellow, face = \"bold\"),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    \n    # Remove gridlines\n    panel.grid = element_blank(),\n    \n    # Margins\n    plot.margin = margin(20, 20, 20, 20)\n  )\n\n\n\n\n\nü§ñ Key differences in my approach:\n\nSeparate geoms for background vs.¬†highlighted lines: This gives me more control over their appearance (alpha, linewidth)\nUsed sec.axis(): Creates a secondary axis for y-axis labels instead of manual annotation‚Äîthis automatically positions them\nDynamic positioning: All positions are calculated from the data, not hardcoded\nUsed expansion(): Modern ggplot2 way to control axis padding\nAdded caption: Credits the data source and visualization creator\nFixed typo: ‚Äúbuy‚Äù instead of ‚Äúby‚Äù\nUsed alpha(): For transparency on background lines\n\nTradeoffs:\n\nOriginal approach: More control over exact positions but requires manual tweaking\nMy approach: More automated but might not position labels perfectly without adjustment\n\nNeither approach is definitively ‚Äúbetter‚Äù‚Äîit depends on whether you value precision (manual positioning) or maintainability (calculated positioning)."
  },
  {
    "objectID": "demos/BigMacIndex.html#synthesis-different-problem-solving-styles",
    "href": "demos/BigMacIndex.html#synthesis-different-problem-solving-styles",
    "title": "25 Years of the Big Mac Index",
    "section": "Synthesis: Different Problem-Solving Styles",
    "text": "Synthesis: Different Problem-Solving Styles\nThe comparison above illustrates two valid approaches to data visualization:\nHuman approach (original): - Iterative and artistic - Manual fine-tuning for pixel-perfect results - Quick, pragmatic solutions (base R subsetting, hardcoded dates) - Relies on visual inspection and adjustment\nLLM approach (Claude‚Äôs version): - Systematic and programmatic - Aims for robustness and reusability - More verbose but self-documenting - Prioritizes reproducibility\nBest practice: Combine both approaches! Use automated methods for the initial layout, then add manual tweaks for publication quality. The original code‚Äôs hand-tuned label positions are exactly what you‚Äôd do after my automated approach got you 90% there."
  },
  {
    "objectID": "demos/BigMacIndex.html#design-insights",
    "href": "demos/BigMacIndex.html#design-insights",
    "title": "25 Years of the Big Mac Index",
    "section": "Design Insights",
    "text": "Design Insights\nBoth versions share these strong design choices:\n\nMcDonald‚Äôs branding: The red-yellow color scheme immediately connects to the subject matter\nMinimalist axes: Removing gridlines lets the data shine\nStrategic highlighting: Only labeling extremes prevents visual clutter\nExtended canvas: Creating space for labels outside the data region\nBold typography: Ensures readability against the colored background"
  },
  {
    "objectID": "demos/BigMacIndex.html#saving-the-plot",
    "href": "demos/BigMacIndex.html#saving-the-plot",
    "title": "25 Years of the Big Mac Index",
    "section": "Saving the Plot",
    "text": "Saving the Plot\n\n\nCode\nggsave(\"bic_mac_2025.png\",\n       width = 7,\n       height = 6,\n       units = \"in\",\n       dpi = 300)"
  },
  {
    "objectID": "demos/BigMacIndex.html#sources",
    "href": "demos/BigMacIndex.html#sources",
    "title": "25 Years of the Big Mac Index",
    "section": "Sources",
    "text": "Sources\n\nWoodall, P. (1986). ‚ÄúBig Mac Index.‚Äù The Economist, September 1986.\nThe Economist. ‚ÄúThe Big Mac Index.‚Äù Available at: https://www.economist.com/big-mac-index\nThe Economist. Big Mac Index GitHub Repository. Available at: https://github.com/TheEconomist/big-mac-data"
  },
  {
    "objectID": "demos/BigMacIndex.html#on-using-llms-for-data-analysis-blogging",
    "href": "demos/BigMacIndex.html#on-using-llms-for-data-analysis-blogging",
    "title": "25 Years of the Big Mac Index",
    "section": "On Using LLMs for Data Analysis Blogging",
    "text": "On Using LLMs for Data Analysis Blogging\nü§ñ Reflective evaluation by Claude\n\nWhat worked well in this collaboration:\nContent generation: LLMs excel at writing contextual introductions, explaining technical concepts, and providing historical background. The sections on PPP theory and the Big Mac Index‚Äôs history add educational value that might be tedious to research and write manually.\nCode commentary: Having an LLM review and comment on code provides a ‚Äúpair programming‚Äù experience. It can spot potential issues (like hardcoded indices), explain design decisions, and suggest alternatives‚Äîall valuable for readers learning data visualization.\nAlternative implementations: Showing two approaches to the same problem (human intuition vs.¬†algorithmic thinking) creates richer learning material than either approach alone.\n\n\nLimitations and cautions:\nAccuracy verification needed: The historical information about the Big Mac Index should be verified against primary sources. LLMs can hallucinate dates, names, or details. (Note: I did search for and cite sources, but independent verification is always good practice.)\nCode execution: I cannot run R code or verify that the plots actually render correctly. The ‚ÄúClaude‚Äôs approach‚Äù code is untested and might have bugs or produce unexpected results. Always run and verify LLM-generated code.\nSubjective judgments: My ‚Äúcommentary‚Äù on the original code reflects general programming principles, but there‚Äôs no objectively ‚Äúright‚Äù way to write this analysis. Your approach may be better for your specific context.\nLoss of authentic voice: Over-relying on LLM-generated prose can make your blog sound generic. The original code and your design decisions are more valuable than any prose I can generate.\n\n\nRecommended workflow:\n\nHuman leads: You write the code, make design decisions, and set the creative direction\nLLM assists: Generate boilerplate explanations, research background information, and provide code review\nHuman curates: Edit LLM output to match your voice, verify all factual claims, and test all code\nTransparency: Mark LLM contributions (as done here) so readers understand the collaborative process\n\n\n\nThe real value:\nThis collaboration model works because it combines human creativity and domain expertise with LLM efficiency at research and documentation. You brought the interesting visualization idea and aesthetic sensibility. I helped structure the narrative and provide alternative perspectives. Neither alone would produce this exact result.\nThe comparison between two coding approaches is particularly valuable‚Äîit gives readers insight into the decision-making process, not just the final solution. This ‚Äúshowing your work‚Äù approach is pedagogically stronger than a single ‚Äúcorrect‚Äù implementation.\nFinal recommendation: Use LLMs as collaborative tools for your blog, not ghostwriters. Your unique perspective and visualization skills are the core value. LLM assistance with context, explanation, and code review enhances that value without replacing it.\n\nOriginal code and visualization by [Your Name] | AI collaboration with Claude (Anthropic) | Generated 2025-12-18"
  },
  {
    "objectID": "demos/bobross.html",
    "href": "demos/bobross.html",
    "title": "Color Frequency",
    "section": "",
    "text": "Every Tuesday, volunteers prepare a data set for people to practice data tidying and plotting skills with R. This is how I‚Äôve interpreted Bob Ross Paintings.\nUpon learning that the producers intentionally limited the show‚Äôs palette to 18 colors, I wanted to find the answer to this question: how often is each color used? As in, what is the sum appearances across all 403 episodes of these 18 colors?\nIt‚Äôs also a great example because this information can‚Äôt be immediately gleaned from the data set. The granularity is at the episode level, meaning 403 episodes of the show are contained in 403 observations in the data. Our target granularity is the color level, meaning we first need to create a data frame of 18 observations before we can build the plot.\nThe information pertaining to colors used and those colors‚Äô hex code has to be messaged out of the source and be organized into a new data frame from which to build a plot."
  },
  {
    "objectID": "demos/bobross.html#data-source",
    "href": "demos/bobross.html#data-source",
    "title": "Color Frequency",
    "section": "",
    "text": "Every Tuesday, volunteers prepare a data set for people to practice data tidying and plotting skills with R. This is how I‚Äôve interpreted Bob Ross Paintings.\nUpon learning that the producers intentionally limited the show‚Äôs palette to 18 colors, I wanted to find the answer to this question: how often is each color used? As in, what is the sum appearances across all 403 episodes of these 18 colors?\nIt‚Äôs also a great example because this information can‚Äôt be immediately gleaned from the data set. The granularity is at the episode level, meaning 403 episodes of the show are contained in 403 observations in the data. Our target granularity is the color level, meaning we first need to create a data frame of 18 observations before we can build the plot.\nThe information pertaining to colors used and those colors‚Äô hex code has to be messaged out of the source and be organized into a new data frame from which to build a plot."
  },
  {
    "objectID": "demos/bobross.html#simple-code-reproduction",
    "href": "demos/bobross.html#simple-code-reproduction",
    "title": "Color Frequency",
    "section": "Simple code reproduction",
    "text": "Simple code reproduction\n\n\nCode\n# library(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n# library(DataExplorer)\nlibrary(tidyr)\nlibrary(extrafont)\nlibrary(ggtext)\nlibrary(tidytuesdayR)\n\n#\ntt = tidytuesdayR::tt_load(\"2023-02-21\")\ndf = tt$bob_ross\n\n# font_import()\nloadfonts(quiet = TRUE)\n\n\n# total times used --------------------------------------------------------\n\ncolornames = c(\"Titanium_White\",\n         \"Bright_Red\",\n         \"Alizarin_Crimson\",\n         \"Van_Dyke_Brown\",\n         \"Cadmium_Yellow\",\n         \"Yellow_Ochre\",\n         \"Phthalo_Blue\",\n         \"Midnight_Black\",\n         \"Sap_Green\",\n         \"Indian_Yellow\",\n         \"Dark_Sienna\",\n         \"Prussian_Blue\",\n         \"Phthalo_Green\",\n         \"Black_Gesso\",\n         \"Burnt_Umber\",\n         \"Liquid_Clear\",\n         \"Liquid_Black\",\n         \"Indian_Red\") |&gt; sort()\n\n\n\n\n## colors\n\n# get_code = function(df, var) {df |&gt; filter({{ var }})}\n# get_code(df, Yellow_Ochre) |&gt; view()\n\ncols = tibble(\n  color_name = colornames,\n  codes = c('#4E1500',\n                     '#000000',\n                     '#DB0000',\n                     '#8A3324',\n                     '#FFEC00',\n                     '#5F2E1F',\n                     '#CD5C5C',\n                     '#FFB800',\n                     '#000000',\n                     '#FFFFFF',\n                     '#000000',\n                     '#0C0040',\n                     '#102E3C',\n                     '#021E44',\n                     '#0A3410',\n                     '#FFFFFF',\n                     '#221B15',\n                     '#C79B00'\n                     )\n)\n\n# aggregate sums of TRUES for each color column to arrive at total counts for each color across all 403 episodes\ncounts = \n  df |&gt; \n  select(10:ncol(df)) |&gt; \n  mutate(across(everything(), ~ sum(.))) |&gt; \n  distinct() |&gt; \n  pivot_longer(1:18,\n               names_to = \"color_name\",\n               values_to = \"count\") |&gt; \n  inner_join(cols) |&gt; \n  arrange(color_name) |&gt; \n  mutate(color_name = str_replace(color_name, \"_\", \" \") |&gt; str_replace(\"_\", \" \"),\n         color_label = if_else(color_name %in% c(\"Titanium White\", \"Liquid Clear\"), \"black\", codes)) \n\n\nOnce we have a data frame titled counts, we can then use ggplot to visualize the data.\n\n\nCode\ncounts |&gt; \n  ggplot(aes(x = count, \n             y = reorder(color_name, count),\n             fill = color_name,\n             label = count)) + \n  geom_col(show.legend = FALSE) +\n  geom_text(aes(y = color_name),\n            show.legend = FALSE,\n            hjust = -.1,\n            family = \"ITC Korinna\",\n            size = 5) +\n  scale_fill_manual(values = counts$codes) +\n  scale_x_continuous(expand = c(0, 0), limits = c(0, 425)) +\n  theme_minimal() +\n  labs(title = \"&lt;b&gt;Color Frequency, Bob Ross' &lt;i&gt;The Joy of Painting&lt;/i&gt;&lt;/b&gt;\",\n       subtitle = \"Across 403 episodes (paintings), how often is each color used?\",\n       caption = \"Tidy Tuesday 21 February 2023&lt;br&gt;&lt;b&gt;solplots.com&lt;/b&gt;\") +\n  theme(text = element_text(family = \"ITC Korinna\",\n                            size = 16,\n                            color = \"black\",\n                            face = \"bold\"),\n        plot.title = element_markdown(),\n        plot.subtitle = element_markdown(),\n        axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = '#FFEDA3'),\n        panel.background = element_rect(fill = \"#FFEDA3\",\n                                        color = \"#FFEDA3\"),\n        plot.caption = element_markdown()) \n\n\n\n\n\nThis is just one way to plot this data. I also liked this interpretation by Art Steinmetz.\n\nGenerated with R R version 4.5.1 (2025-06-13 ucrt) on 2025-12-18"
  },
  {
    "objectID": "demos/CalFires.html",
    "href": "demos/CalFires.html",
    "title": "California forest fires acres battled by agency",
    "section": "",
    "text": "My interpretation of forest fire data courtesy of CalFire. This page is currently undergoing updates.\n\nLibraries\nFor two reasons (the fact that our data lives in a PDF file plus a few other sources, and the fact that I‚Äôm using a pre-defined color palette) we need three packages in addition to the tidyverse in order to produce the plot.\n\npalettetown: a package full of color palettes derived from Pokemon and inspired by pokepalettes; each Pokemon is represented in the package, but my favorite is Charizard and happens to go along with the theme of the data.\npdftools: this data is stored in a pdf file, so this is an opportunity to show how to use R to replicate the process of extracting the data from the file and into a data frame from which to be plotted.\njanitor: a very helpful package for restructuring dirty data frames, such as those extracted from pdf files.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(palettetown)\nlibrary(pdftools)\nlibrary(janitor)\n\n\n\n\nData Processing\nTo begin the process of extracting the data from the pdf, let‚Äôs instantiate some objects.\n\n\nCode\npdf = \"pdfs/fires-acres-all-agencies-thru-2018.pdf\"\ntib = pdf_data(pdf)[[1]] \n\ntib\n\n\n# A tibble: 355 √ó 6\n   width height     x     y space text         \n   &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;        \n 1    88     22   153    18 TRUE  California   \n 2    80     22   246    18 TRUE  Wildfires    \n 3    32     22   331    18 TRUE  and          \n 4    48     22   368    18 TRUE  Acres        \n 5    25     22   422    18 TRUE  for          \n 6    21     22   453    18 TRUE  all          \n 7   110     22   479    18 FALSE Jurisdictions\n 8    18      9   145    43 TRUE  CAL          \n 9    21      9   167    43 FALSE FIRE         \n10    26      9   116    57 FALSE FIRES        \n# ‚Ñπ 345 more rows\n\n\n\n\nCode\npartial = tib %&gt;% \n  .[23:278, c(4,6)] %&gt;% \n  group_by(y) %&gt;% \n  nest() %&gt;% \n  pull(data) %&gt;% \n  bind_cols() %&gt;% \n  row_to_names(1) %&gt;% \n  add_column(source = c(\"fires_cal\",\n                        \"acres_cal\",\n                        \"fires_fed\",\n                        \"acres_fed\",\n                        \"fires_local\",\n                        \"acres_local\",\n                        \"fires_total\"),\n             .before = \"1987\") %&gt;% \n  pivot_longer(cols = -1) %&gt;% \n  pivot_wider(names_from = \"source\",\n              values_from = \"value\") %&gt;% \n  dplyr::rename(year = name)\n\nmore = tib %&gt;% \n  .[323:354, 6] %&gt;% \n  select(acres_total = text)\n\ndf = partial %&gt;% \n  bind_cols(more) %&gt;% \n  mutate(across(2:9, ~ as.numeric(eeptools::decomma(.))),\n         year = as.numeric(partial$year)) \n\nrest = tibble(\n  year = c(2019, 2020, 2021, 2022, 2023),\n  fires_cal = c(3086, # 2019 value\n                3501, # 2020 value \n                3054, # ...\n                3333, \n                3236),\n  acres_cal = c(129914, # 2019 value\n                1458881, # 2020 value \n                279703, # ... \n                70933, \n                24971),\n  fires_fed = c(997 + 156 + 34 + 80 + 15 + 2, # 2019 value\n                1421 + 217 + 79 + 97 + 13 + 5, # 2020 value \n                1267 + 183 + 115 + 83 + 20 + 1, # ...\n                934 + 79 + 50 + 36 + 13 + 19,\n                1022 + 82 + 84 + 65 + 8 + 7),\n  acres_fed = c(1112399 + 8539 + 111+334 + 2754 + 33, # 2019 value\n                2520946 + 142201 + 76796 + 22210 + 45 + 11476, # 2020 value \n                2029239 + 30145 + 109420 + 98793 + 552 + 1000, # ...\n                10932 + 234624 + 3240 + 6864 + 156 + 193 + 128,\n                187255 + 1557 + 90251 + 201 + 8),\n  fires_local = c(2370 + 408,  # 2019 value\n                  2849 + 466, # 2020 value \n                  2420 + 253, # ... \n                  2642 + 371, \n                  2508 + 374),\n  acres_local = c(7220 + 15981,  # 2019 value\n                  17062 + 54762, # 2020 value \n                  13828 + 6706, # ... \n                  4288 + 10932, \n                  4936 + 18976),\n  fires_total = c(7148, 8648, 7396, 7490, NA),\n  acres_total = c(277285, 4304379, 2569386, 362455, NA)\n)\n\nfires &lt;- \n  df %&gt;% \n  rbind(rest) %&gt;% \n  pivot_longer(cols = fires_cal:acres_total, names_to = \"type\") %&gt;% \n  filter(str_starts(type, \"acres\") & !str_ends(type, \"total\")) %&gt;% \n  mutate(\n    type = case_when(\n      type == \"acres_local\" ~ \"Local\",\n      type == \"acres_cal\" ~ \"State\",\n      type == \"acres_fed\" ~ \"Federal\"\n    ) %&gt;% \n      as.factor(.) %&gt;% \n      recode_factor(\n        ., \n        \"Local\" = \"Local\",\n        \"State\" = \"State\",\n        \"Federa\" = \"Federal\"))\n\n\n\n\nColor scheme\nPulling our color scheme from the palettetown package that produces hex codes corresponding with first-generation Pokemon. We‚Äôll select Charizard.\n\n\nCode\nchar &lt;- palettetown::ichooseyou(\"charizard\")\nextrafont::loadfonts()\n\n\n\n\nPlotting\n\n\nCode\np &lt;- \n  fires %&gt;% \n  ggplot(aes(x = year, y = value/1000, fill = type)) +\n  geom_col(color = \"white\",\n           width = 1) +\n  labs(title = \"Califoria forest fires acres battled by agency\",\n       subtitle = \"Source: CalFire\",\n       x = \"\",\n       y = \"\",\n       fill = \"\",\n       caption = \"solplots.com\") +\n  scale_fill_manual(values = c(\"Federal\" = char[11],\n                               \"State\" = char[8],\n                               \"Local\" = char[2])) +\n  scale_x_continuous(expand = c(0, 0),\n                     breaks = seq(1987, 2023, 4)) +\n  scale_y_continuous(expand = c(0, 0),\n                     limits = c(0, 4700)) +\n  annotate(\"text\", \n           x = rep(1985, 5),\n           y = seq(150, 4150, 1000),\n           label = c(0:3, \"4 million acres\"),\n           color = \"white\",\n           family = \"Garamond\",\n           hjust = 0,\n           size = 5) +\n  theme(panel.grid.minor.x = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid = element_line(color =\"white\"),\n        panel.border = element_blank(),\n        plot.title = element_text(size = rel(1.3),\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = rel(1.2)), \n        text = element_text(color = \"white\",\n                            family = \"Georgia\"),\n        axis.text.y = element_blank(),\n        axis.text.x = element_text(color = \"white\",\n                                   size = rel(1.3)),\n        panel.background = element_rect(fill = char[4]),\n        plot.background = element_rect(fill = char[4]),\n        legend.background = element_rect(fill = char[4],\n                                         color = char[4]),\n        legend.title = element_blank(),\n        legend.text = element_text(size = rel(1.1)),\n        legend.position = \"inside\",\n        legend.position.inside = c(0.26, 0.965),\n        legend.direction = \"horizontal\",\n        axis.ticks = element_blank(),\n        plot.caption = element_text(face = \"bold\",\n                                    size = 10),\n        plot.margin = unit(c(5,5,3,0), \"mm\"))\n\np"
  },
  {
    "objectID": "demos/CensusDc.html",
    "href": "demos/CensusDc.html",
    "title": "Median Income and Race in the District of Columbia",
    "section": "",
    "text": "Suppose you need to ‚Äúpull‚Äù U.S. Census data and ‚Äúrun‚Äù some summary statistics. Consider this approach."
  },
  {
    "objectID": "demos/CensusDc.html#packages",
    "href": "demos/CensusDc.html#packages",
    "title": "Median Income and Race in the District of Columbia",
    "section": "Packages",
    "text": "Packages\n\ntidyverse (ggplot2, dplyr) for plotting and wrangling data\nsf (simple features) - I‚Äôll only be using one function geom_sf() to plot the map with ggplot2, but this package is part of the geocomputation ecosystem of R.\ntidycensus - an R package by Kyle Walker that allows users to interface with a select number of the US Census Bureau‚Äôs data APIs and return tidyverse-ready data frames, optionally with simple feature geometry included.\n\n\n\nCode\n# install.packages(c(\"tidyverse\", \"extrafont\", \"tidycensus\", \"sf\"))\n\nlibrary(tidyverse)\nlibrary(tidycensus)\n\nextrafont::loadfonts()"
  },
  {
    "objectID": "demos/CensusDc.html#intiating-api-connection",
    "href": "demos/CensusDc.html#intiating-api-connection",
    "title": "Median Income and Race in the District of Columbia",
    "section": "Intiating API Connection",
    "text": "Intiating API Connection\nTo begin pulling data from the U.S. Census into R, you must submit your email to the Census in order to receive an API key. I have hidden my key for this demo.\nTake your key and instantiate it in your R session like so:\n\n\nCode\ncensus_api_key(my_key)\noptions(tigris_use_cache = TRUE)"
  },
  {
    "objectID": "demos/CensusDc.html#data-pull",
    "href": "demos/CensusDc.html#data-pull",
    "title": "Median Income and Race in the District of Columbia",
    "section": "Data Pull",
    "text": "Data Pull\nTo download the data we need (median household income at the tract level of White, Black, Asian and Hispanic residents of DC) we will use get_acs().\n\n\nCode\ndc &lt;- \n  get_acs(\n    geography = \"tract\",\n    # this could be block, block group, tract, county, etc.\n    variables = c(\n      \"White\" = \"B19013A_001\",\n      # white\n      \"Black\" = \"B19013B_001\",\n      # black\n      \"Asian\" = \"B19013D_001\",\n      # asian\n      \"Hispanic \" = \"B19013I_001\"\n      # hispanic\n    ),\n    # this could be whole plotting and thotting topic on its own...\n    state = \"DC\",\n    # DC counts as a state here\n    year = 2023,\n    # I first created this plot when 2022 had the most recent data\n    geometry = TRUE\n    # we want to return an sf object (similar to tidy table/tibble that \n    # also has a geometric component)\n  ) |&gt;\n  replace_na(list(estimate = 0))\n  # if it's NA it equals 0 in our situation"
  },
  {
    "objectID": "demos/CensusDc.html#cleaning-data-set",
    "href": "demos/CensusDc.html#cleaning-data-set",
    "title": "Median Income and Race in the District of Columbia",
    "section": "Cleaning Data Set",
    "text": "Cleaning Data Set\nOk, so we know that nobody lives on the National Mall (which is its own census tract), so if we included that row in our map, it‚Äôll be filled in just like the others.\nI took the creative liberty to decide to remove all tracts where the value of median household income is 0.\n\n\nCode\nremove &lt;- \n  dc |&gt; \n  filter(estimate == 0) |&gt; \n  # all rows where estimate is 0\n  group_by(GEOID) |&gt; \n  # create groups based on the ID of the geometric feature\n  count() |&gt; \n  # for each GEOID, how many rows have a 0 in estimate?\n  filter(n == 4) |&gt; \n  # all rows where value is 4 (four variables, four zeroes)\n  pull(GEOID) \n  # retrieve GEOIDs of tracts we want to remove\n\ndc_clean &lt;-\n  dc |&gt; \n  filter(!GEOID %in% remove)\n\n\nWe can observe the first few rows of this data to get an idea of what‚Äôs going on:\n\n\nCode\ndc_clean |&gt;\n  head(4) |&gt;\n  knitr::kable(\"html\") |&gt; \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nGEOID\nNAME\nvariable\nestimate\nmoe\ngeometry\n\n\n\n\n11001010300\nCensus Tract 103; District of Columbia; District of Columbia\nWhite\n193472\n45702\nPOLYGON ((-77.03636 38.9748...\n\n\n11001010300\nCensus Tract 103; District of Columbia; District of Columbia\nBlack\n99545\n53794\nPOLYGON ((-77.03636 38.9748...\n\n\n11001010300\nCensus Tract 103; District of Columbia; District of Columbia\nAsian\n0\nNA\nPOLYGON ((-77.03636 38.9748...\n\n\n11001010300\nCensus Tract 103; District of Columbia; District of Columbia\nHispanic\n0\nNA\nPOLYGON ((-77.03636 38.9748...\n\n\n\n\n\n\n\nHere are four rows that each refer to Census Tract 103 in the District of Columbia. The variable column describes these rows as containing the median household income by race, while estimate shows the actual value. We can see that the White median household income for Tract 103 was estimated to be $193,333 in 2022; for Black households in that same tract the estimate is $85,758, and for Hispanic households it was $64,233. It appears that there are not enough Asian households in this tract for there to be a reliable estimate."
  },
  {
    "objectID": "demos/CensusDc.html#plotting-the-map",
    "href": "demos/CensusDc.html#plotting-the-map",
    "title": "Median Income and Race in the District of Columbia",
    "section": "Plotting the Map",
    "text": "Plotting the Map\nAs mentioned earlier, we will use the geom_sf() function to draw a map within the ggplot framework.\n\n\nCode\nmap &lt;- \n  dc_clean |&gt;\n  # filter for rows where not GEOID are in remove (GEOID not in remove)\n  ggplot(aes(fill = estimate)) + \n  # start ggplot object\n  geom_sf() + \n  # apply sf geom\n  labs(title = \"Race and Income in DC\",\n       subtitle = \"2023 Inflation-adjusted USD\\nMedian Household Income\",\n       caption = \"Source: 2019-2023 5-year\\nAmerican Community Servey\") +\n  # set title, subtitle and caption (using new line character \"\\n\")\n  scale_fill_viridis_c(labels = scales::dollar,\n                       option = \"inferno\",\n                       direction = -1) +\n  # apply a colorblind-friendly color palette\n  # we also want dollar signs on our legend\n  # the \"inferno\" color palette\n  # reverse the high/low direction from default\n  facet_wrap(~ variable, nrow = 2) +\n  # instead of one DC\n  # we get four DC's (for each variable, what is being faceted)\n  # ensure there are 2 rows of plots\n  theme_bw() + \n  # black and white theme \n  theme(panel.grid = element_blank(),\n        # remove all grid lines (graticules are not necessary here)\n        axis.text = element_blank(),\n        # no need for degrees longitude and latitude\n        axis.ticks = element_blank(),\n        # ticks also go\n        legend.title = element_blank(),\n        # a title for the legend is redundant so it goes as well\n        strip.background = element_rect(fill = \"white\"),\n        # change from grey\n        legend.spacing.y = unit(.3, \"cm\"),\n        # adjust spacing a tiny bit\n        plot.title = element_text(face = \"bold\"),\n        # put the title to bold\n        text = element_text(family = \"Garamond\"),\n        # change font to Garamond\n        plot.caption = element_text(hjust = 0)\n        # left adjusted caption\n  )\n\nmap"
  },
  {
    "objectID": "demos/CensusDc.html#saving-your-map",
    "href": "demos/CensusDc.html#saving-your-map",
    "title": "Median Income and Race in the District of Columbia",
    "section": "Saving Your Map",
    "text": "Saving Your Map\nNow that you‚Äôve successfully plotted Census data on map, you can save the image file to your computer using ggsave() (code sample below):\n\n\nCode\n# ggsave(\"median_income_by_race_dc_census_tracts.png\",\n#        plot = map,\n#        device = \"png\",\n#        width = 6.3,\n#        height = 4.3,\n#        unit = \"in\")\n\n\nIf you found this tutorial helpful, try this out on your own."
  },
  {
    "objectID": "demos/ombReceipts.html",
    "href": "demos/ombReceipts.html",
    "title": "Sources of U.S. Federal Revenue Over The Years Part 1",
    "section": "",
    "text": "Inspiration\nI encountered this Reddit post about a year ago and felt compelled to reproduce it. The plot below is what this blog post is going to reproduce.\n\nThis is the first of a two-part series on this data set and plot. In the next post I‚Äôll show how to add animation to the plot.\nThere are a few aspects of this plot that I really like, and I can learn from this plot how to deploy these visualization tactics in future situations:\n\nUse of proportional area chart\n\n\n\nLegend arrangement\nCall-out of 2023 values (2024 has been published as well so theoretically this plot could use an update)\nColor scheme\n\n\n\nData Source\nIn the year 2025 it is such a pleasure to work with the U.S. Government when it comes to data matters; just don‚Äôt report bad numbers!\nHere is where you can find historical budget tables. Clicking on the workbook, we want the data in the tab titled ‚Äúhist02z1‚Äù.\nThis table has got a bunch of (what I like to call) Excel formatting bullshit that we‚Äôre going to have to untangle.\n\n\nSimple Code Reproduction\nThis first chunk of code loads in the libraries needed to plot, reads in our file of data, creates the color palette for the plot. Additionally, it extracts the specific rows and columns in the matrix that we need for our plot.\n\n\nCode\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(extrafont)\n\nfile &lt;- read_csv(\"data/hist02z1_fy2025(Table).csv\")\npal &lt;- c(\"#952929\", \"#eb3c33\", \"#ffca41\", \"#485ba6\", \"#0d0d0d\")\n\nmat &lt;- file[-c(1, 3, 95:103), -c(5, 6, 9:11)]\n\n\nThe following chunk processes the matrix into a data frame suitable for the type of plot we are to achieve.\n\n\nCode\ndf &lt;- \n  mat |&gt; \n  row_to_names(1) |&gt; \n  clean_names() |&gt; \n  mutate(\n    across(\n      everything(),\n      ~ . |&gt; \n        str_remove(\",\") |&gt; \n        str_remove(\",\") |&gt; \n        as.numeric())\n    ) |&gt; \n  pivot_longer(\n    2:6, \n    names_to = \"revenue_type\", \n    values_to = \"millions\"\n    )\n\nfin &lt;- \n  df |&gt; \n  filter(fiscal_year &gt; 1940) |&gt; \n  mutate(\n    type = case_match(\n      revenue_type, \n      \"other_3\" ~ \"Other\",\n      \"excise_taxes_2\" ~ \"Excise taxes\",\n      \"social_insurance_and_retirement_receipts_2\" ~ \"Social insurance and retirement receipts\",\n      \"corporation_income_taxes_1\" ~ \"Corporation income taxes\", \n      \"individual_income_taxes\" ~ \"Individual income taxes\")\n    ) |&gt; \n  mutate(\n    type = factor(\n      type, \n      levels = c(\"Individual income taxes\",\n                 \"Corporation income taxes\",\n                 \"Social insurance and retirement receipts\",\n                 \"Excise taxes\",\n                 \"Other\")))\n\n\nNow that we have an ideal data frame, we can plot.\n\n\nCode\nfin |&gt; \n  ggplot(\n    aes(x = fiscal_year, \n        y = millions, \n        fill = type)) +\n  geom_area(position = \"fill\") +\n  scale_x_continuous(\n    breaks = seq(1942, 2022, 8),\n    expand = c(0, 0),\n    limits = c(1940, 2030)\n  ) +\n  scale_y_continuous(\n    labels = scales::percent,\n    breaks = seq(0, 1, .2),\n    expand = c(.015, 0)\n    ) +\n  scale_fill_manual(values = pal) +\n  labs(\n    title = \"U.S. Federal Revenue Sources\",\n    subtitle = \"Distribution of federal revenue by source (1940 - 2023, by fiscal year)\",\n       caption = \"Source: Office of Management and Budget\") +\n  annotate(\"label\",\n           x = 2026.5,\n           y = c(.75, .46, .11, .03, .3),\n           label = c(\"49%\", \"9.4%\", \"1.7%\", \"3.5%\", \"36.4%\"),\n           family = \"Garamond\",\n           size = 4,\n           fill = c(pal[1], pal[2], pal[4], pal[5], pal[3]),\n           color = c(rep(\"grey95\", 4), \"black\")) +\n  annotate(\"text\",\n           x = 2026.5,\n           y = 1.05,\n           label = \"2023:\",\n           family = \"Garamond\",\n           size = 4) +\n  coord_cartesian(clip = \"off\") |&gt; \n  guides(fill = guide_legend(nrow = 2)) +\n  theme_minimal() +\n  theme(legend.position = \"top\",\n        axis.title = element_blank(),\n        axis.text = element_text(face = \"bold\",\n                                 family = \"Garamond\",\n                                 size = 14),\n        legend.title = element_blank(),\n        legend.text = element_text(family = \"Garamond\",\n                                   size = 11.2),\n        legend.key.width = unit(.35, \"cm\"),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        plot.title = element_text(face = \"bold\",\n                                  family = \"Garamond\",\n                                  size = 21),\n        plot.title.position = \"plot\",\n        legend.margin = margin(0, 0, 0, 0), # turned off for alignment\n        legend.justification.top = \"left\",\n        legend.location = \"plot\",\n        plot.subtitle = element_text( family = \"Garamond\",\n                                     size = 19,\n                                     color = \"grey30\"),\n        plot.background = element_rect(fill = \"grey95\"),\n        plot.caption = element_text(face = \"bold\",\n                                    family = \"Garamond\",\n                                    size = 12)) \n\n\n\n\n\nI made the decision to color each of the 2023 value labels the same color as their associated area color to make it easier to read. I also did not get the legend ordering to match"
  },
  {
    "objectID": "demos/ombReceipts_Animate.html",
    "href": "demos/ombReceipts_Animate.html",
    "title": "Sources of U.S. Federal Revenue Over The Years Part 2",
    "section": "",
    "text": "Introduction\nPlease refer to Part 1 for context on how to reproduce the static version of an area chart that shows the distribution of U.S. Federal Revenue Sources over time. This post will explain how to animate the data so that the colors appear to fill in the plot as the years increase.\n\nThe R package that enables us to create animations and store them as .gif files is the camcorder package.\nHow we are going to achieve the animation is that we will essentially publish roughly 80 plot images (one for each year between 1941 and 2023) and then stitch them together (in order) to form a gif. The package assists us in stitching images together.\n\n\nSimple Code Reproduction\nThe following is all of the code from Part 1 up until the portion that actually creates the plot with a slight alteration at the end: we mutate an additional column that shows each group‚Äôs proportion for every year.\n\n\nCode\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(extrafont)\nlibrary(camcorder)\n\nfonts()\n\nfile &lt;- read_csv(\"data/hist02z1_fy2025(Table).csv\")\npal &lt;- c(\"#952929\", \"#eb3c33\", \"#ffca41\", \"#485ba6\", \"#0d0d0d\")\n\nmat &lt;- file[-c(1, 3, 95:103), -c(5, 6, 9:11)]\n\ndf &lt;- \n  mat |&gt; \n  row_to_names(1) |&gt; \n  clean_names() |&gt; \n  mutate(\n    across(\n      everything(),\n      ~ . |&gt; \n        str_remove(\",\") |&gt; \n        str_remove(\",\") |&gt; \n        as.numeric())\n    ) |&gt; \n  pivot_longer(\n    2:6, \n    names_to = \"revenue_type\", \n    values_to = \"millions\"\n    )\n\nfin &lt;- \n  df |&gt; \n  filter(fiscal_year &gt; 1940) |&gt; \n  mutate(\n    type = case_match(\n      revenue_type, \n      \"other_3\" ~ \"Other\",\n      \"excise_taxes_2\" ~ \"Excise taxes\",\n      \"social_insurance_and_retirement_receipts_2\" ~ \"Social insurance and retirement receipts\",\n      \"corporation_income_taxes_1\" ~ \"Corporation income taxes\", \n      \"individual_income_taxes\" ~ \"Individual income taxes\")\n    ) |&gt; \n  mutate(\n    type = factor(\n      type, \n      levels = c(\"Individual income taxes\",\n                 \"Corporation income taxes\",\n                 \"Social insurance and retirement receipts\",\n                 \"Excise taxes\",\n                 \"Other\"))) |&gt;\n  group_by(fiscal_year) |&gt; \n  mutate(pct_label = paste0(round((millions / sum(millions))*100,  digits = 2), \"%\")) |&gt; \n  ungroup()\n\n\nTo produce plot images for each year between 1941 and 2023, let‚Äôs create a function that takes a data frame and a year and produces a plot image. We can create the function animate_plot() to accomplish this.\n\n\nCode\nanimate_plot &lt;- function(df, filter_year) {\n  \n  filt_df &lt;- \n    df |&gt; \n    filter(fiscal_year &lt;= filter_year) \n  \n  labels &lt;- \n    df |&gt; \n    filter(fiscal_year == filter_year + 1) |&gt; \n    pull(pct_label)\n  \n  labels &lt;- labels[c(1:2, 4:5, 3)]\n  \n  # filter_year_label &lt;- expression(underline(bold(paste0(filter_year, \":\"))))\n  \n  filt_df |&gt; \n    ggplot(\n      aes(x = fiscal_year, \n          y = millions, \n          fill = type)) +\n    geom_area(position = \"fill\") +\n    scale_x_continuous(\n      breaks = seq(1942, 2022, 8),\n      expand = c(0, 0),\n      limits = c(1940, 2030)\n    ) +\n    scale_y_continuous(\n      labels = scales::percent,\n      breaks = seq(0, 1, .2),\n      expand = c(.015, 0)\n    ) +\n    scale_fill_manual(values = pal) +\n    labs(\n      title = \"U.S. Federal Revenue Sources\",\n      subtitle = \"Distribution of federal revenue by source (1940 - 2023, by fiscal year)\",\n      caption = \"Source: Office of Management and Budget\") +\n    annotate(\"label\",\n             x = filter_year + 4.5,\n             y = c(.75, .46, .11, .03, .3),\n             label = labels,\n             family = \"Garamond\",\n             size = 4,\n             fill = c(pal[1], pal[2], pal[4], pal[5], pal[3]),\n             color = c(rep(\"grey95\", 4), \"black\")) +\n    annotate(\"text\",\n             x = filter_year + 4.5,\n             y = 1.05,\n             label = filter_year + 1,\n             family = \"Garamond\",\n             size = 4.4) +\n    guides(fill = guide_legend(nrow = 2)) +\n    theme_minimal() +\n    theme(legend.position = \"top\",\n          axis.title = element_blank(),\n          axis.text = element_text(face = \"bold\",\n                                   family = \"Garamond\",\n                                   size = 14),\n          legend.title = element_blank(),\n          legend.text = element_text(family = \"Garamond\",\n                                     size = 11.2),\n          legend.key.width = unit(.35, \"cm\"),\n          axis.ticks = element_blank(),\n          panel.grid = element_blank(),\n          plot.title = element_text(face = \"bold\",\n                                    family = \"Garamond\",\n                                    size = 18),\n          plot.title.position = \"plot\",\n          legend.margin = margin(0, 0, 0, 0), # turned off for alignment\n          legend.justification.top = \"left\",\n          legend.location = \"plot\",\n          plot.subtitle = element_text( family = \"Garamond\",\n                                        size = 17,\n                                        color = \"grey30\"),\n          plot.background = element_rect(fill = \"grey95\"),\n          plot.caption = element_text(face = \"bold\",\n                                      family = \"Garamond\",\n                                      size = 12)) \n  \n}\n\n\nLet‚Äôs take a look at an example year with this function‚Ä¶\n\n\nCode\nanimate_plot(fin, 1973)\n\n\n\n\n\nWe see all the data up until 1974, with x-axis showing the remaining years. And in this image, we have the 1974 numbers called out. This is because when we entered the year 1973 in our function, R only plots data up to and including the year 1973, and the function then determines what the 1974 numbers are and annotates them in the field just beyond the right edge of the area geometry.\nGenerating plot images for each year and stitching them together in order will produce a moving image that shows the area geometry moving to the right for each year. How can we loop through our written function animate_plot() to produce a plot for each year? We‚Äôll use the map() function from the purrr and utilize its functional programming toolkit.\n\n\nCode\nmap(1973:1975, ~ animate_plot(fin, .))\n\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\nEssentially (in our case), we want to loop the function animate_plot() over a vector of years in sequential order. I showed how to use map() in the code chunk above just for 1973, 1974 and 1975 (1973:1975 produces a vector of every number between 1973 and 1975). Therefore, to produce every plot for our time frame we can write map(1942:2022, ~ animate_plot(fin, .)). I‚Äôm not going to run this code on this page and have 81 plot images outputted here.\n\n\nAnimation\nTo utilize the capabilities of the camcorder package, we would run the following code:\n\n\nCode\n# gg_record(\n#   dir = paste0(getwd(), \"/animate_over_years\"), # where to save the recording\n#   device = \"png\", # device to use to save images\n#   width = 7.3,      # width of saved image\n#   height = 5.95,     # height of saved image\n#   units = \"in\",   # units for width and height\n#   dpi = 300       # dpi to use when saving image\n# )\n# \n# map(1942:2022, ~ animate_plot(fin, .))\n# \n# gg_playback(\n#   name = paste0(getwd(), \"/animate_over_years\", \"/animate_over_years.gif\"),\n#   first_image_duration = 5,\n#   last_image_duration = 15,\n#   frame_duration = .15,\n#   image_resize = 800,\n#   background = \"white\"\n# )\n\n\nThe first function, gg_record() initiates a session in which R knows to immediately save as an image every plot that is generated. Once the recording session begins, we can loop over our plot generating function to produce each of the plots we need an in the order we need them. Once each plot is generated and saved to disk, the function gg_playback() takes each image produced and generates a .gif file. You can adjust how long the gif stays on the first and last image as well as how long to stay on each image in between.\nI‚Äôm not running this code for the post because but just un-comment the cell.\nPlease refer to the package repo for more information about the package."
  },
  {
    "objectID": "demos/PewMediaStudy.html",
    "href": "demos/PewMediaStudy.html",
    "title": "Reproducing Pew Research‚Äôs ‚ÄòHow the audiences of 30 major news sources differ in their levels of education‚Äô",
    "section": "",
    "text": "Pew‚Äôs research into the level of higher education among the different audiences of 30 major media outlets includes an interesting bar plot that I would like to reproduce.\nThe article I‚Äôm referencing can be found here.\nIt is a horizontal bar plot with 30 discrete media outlets on the y-axis ordered by their proportion of audience that has obtained a bachelor‚Äôs degree or even more education.\nThis is an interesting plot to reproduce because it expresses themes of meritocracy and how there can be hierarchies within many areas of life, such as media consumption, extracted and laid out for us to see where we may fall. It is definitely an interesting choice for the researchers at Pew not to ask the survey participants if they also get their news from TikTok (either from these very media outlets‚Äô own accounts or from independent journalists reporting stories on the app).\nThis plot feels very similar to the kinds of plots Pew usually produces, so I hope you find my post here informative for how we can take the data they provide us and turn it into this plot.\n\nPackage Libraries\nMuch like the CalFire situation where their data is stored in a PDF file, Pew let‚Äôs you also view a PDF file that organizes the data in a tidy format for plotting. This is nice for us because aside from bring this data in, we won‚Äôt have to change anything beyond how it looks already.\nWe want tidyverse and extrafont (as per usual) and ggtext (a helper package on top of ggplot2).\n\n\nAssist from ClaudeAI\nI decided to enlist the help from Claude to take the string of text in the pdf to become a tidy data frame (aka ‚Äútibble‚Äù) that can be used directly to plot.\nPrompt 1 essentially takes the string I copied that includes all the relevant data and use its knowledge of R code to create a script that manually creates a tibble. Prompt 2 takes the same string but uses the stringr library within tidyverse to build out a process that parses the string itself and creates the same data frame result.\nThe string I used is thus:\n‚ÄúEducation levels of U.S. audiences for 30 major news sources Among U.S. adults who regularly get news from ___, % with each level of education College+ Some college High school or less The Atlantic 62% 23% 14% Axios 59 21 19 NPR 58 27 14 The New York Times 56 24 19 The Guardian 53 28 18 The Wall Street Journal 53 26 21 Politico 52 27 20 The Associated Press 52 28 20 BBC News 49 27 23 The Washington Post 47 30 22 HuffPost 47 28 25 The Hill 45 25 29 Forbes 42 30 28 PBS 41 29 30 CNN 38 28 33 New York Post 38 33 29 The Daily Wire 37 37 26 MSNBC 37 30 33 USA Today 34 30 36 Breitbart 34 32 34 Newsweek 32 32 35 NBC News 32 29 39 ABC News 31 30 39 CBS News 30 30 40 Tucker Carlson Network 28 33 40 Newsmax 28 35 37 Fox News 27 30 42 The Joe Rogan Experience 27 35 38 Telemundo 16 26 56 Univision 15 26 57‚Äù\n\nPrompt 1 Code\n\n\nCode\n# method 1 from Claude ----\n\n# LLM to parse the text and use its knowledge of R code to output the following:\n\nlibrary(dplyr)\nlibrary(tibble)\n\n# Create tibble from the news sources education data\nnews_education &lt;- tribble(\n  ~news_source, ~college_plus, ~some_college, ~high_school_or_less,\n  \"The Atlantic\", 62, 23, 14,\n  \"Axios\", 59, 21, 19,\n  \"NPR\", 58, 27, 14,\n  \"The New York Times\", 56, 24, 19,\n  \"The Guardian\", 53, 28, 18,\n  \"The Wall Street Journal\", 53, 26, 21,\n  \"Politico\", 52, 27, 20,\n  \"The Associated Press\", 52, 28, 20,\n  \"BBC News\", 49, 27, 23,\n  \"The Washington Post\", 47, 30, 22,\n  \"HuffPost\", 47, 28, 25,\n  \"The Hill\", 45, 25, 29,\n  \"Forbes\", 42, 30, 28,\n  \"PBS\", 41, 29, 30,\n  \"CNN\", 38, 28, 33,\n  \"New York Post\", 38, 33, 29,\n  \"The Daily Wire\", 37, 37, 26,\n  \"MSNBC\", 37, 30, 33,\n  \"USA Today\", 34, 30, 36,\n  \"Breitbart\", 34, 32, 34,\n  \"Newsweek\", 32, 32, 35,\n  \"NBC News\", 32, 29, 39,\n  \"ABC News\", 31, 30, 39,\n  \"CBS News\", 30, 30, 40,\n  \"Tucker Carlson Network\", 28, 33, 40,\n  \"Newsmax\", 28, 35, 37,\n  \"Fox News\", 27, 30, 42,\n  \"The Joe Rogan Experience\", 27, 35, 38,\n  \"Telemundo\", 16, 26, 56,\n  \"Univision\", 15, 26, 57\n)\n\n# Display the tibble\nprint(news_education)\n\n\n# A tibble: 30 √ó 4\n   news_source             college_plus some_college high_school_or_less\n   &lt;chr&gt;                          &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n 1 The Atlantic                      62           23                  14\n 2 Axios                             59           21                  19\n 3 NPR                               58           27                  14\n 4 The New York Times                56           24                  19\n 5 The Guardian                      53           28                  18\n 6 The Wall Street Journal           53           26                  21\n 7 Politico                          52           27                  20\n 8 The Associated Press              52           28                  20\n 9 BBC News                          49           27                  23\n10 The Washington Post               47           30                  22\n# ‚Ñπ 20 more rows\n\n\nCode\n# Optional: View structure and summary\nglimpse(news_education)\n\n\nRows: 30\nColumns: 4\n$ news_source         &lt;chr&gt; \"The Atlantic\", \"Axios\", \"NPR\", \"The New York Time‚Ä¶\n$ college_plus        &lt;dbl&gt; 62, 59, 58, 56, 53, 53, 52, 52, 49, 47, 47, 45, 42‚Ä¶\n$ some_college        &lt;dbl&gt; 23, 21, 27, 24, 28, 26, 27, 28, 27, 30, 28, 25, 30‚Ä¶\n$ high_school_or_less &lt;dbl&gt; 14, 19, 14, 19, 18, 21, 20, 20, 23, 22, 25, 29, 28‚Ä¶\n\n\nCode\nsummary(news_education)\n\n\n news_source         college_plus    some_college   high_school_or_less\n Length:30          Min.   :15.00   Min.   :21.00   Min.   :14.00      \n Class :character   1st Qu.:31.25   1st Qu.:27.00   1st Qu.:21.25      \n Mode  :character   Median :38.00   Median :29.00   Median :29.50      \n                    Mean   :40.00   Mean   :28.97   Mean   :30.53      \n                    3rd Qu.:51.25   3rd Qu.:30.00   3rd Qu.:37.75      \n                    Max.   :62.00   Max.   :37.00   Max.   :57.00      \n\n\nCode\n# Optional: Sort by college+ percentage (descending)\nnews_education_sorted &lt;- news_education %&gt;%\n  arrange(desc(college_plus))\n\nprint(\"Sorted by College+ percentage:\")\n\n\n[1] \"Sorted by College+ percentage:\"\n\n\nCode\nprint(news_education_sorted)\n\n\n# A tibble: 30 √ó 4\n   news_source             college_plus some_college high_school_or_less\n   &lt;chr&gt;                          &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n 1 The Atlantic                      62           23                  14\n 2 Axios                             59           21                  19\n 3 NPR                               58           27                  14\n 4 The New York Times                56           24                  19\n 5 The Guardian                      53           28                  18\n 6 The Wall Street Journal           53           26                  21\n 7 Politico                          52           27                  20\n 8 The Associated Press              52           28                  20\n 9 BBC News                          49           27                  23\n10 The Washington Post               47           30                  22\n# ‚Ñπ 20 more rows\n\n\nCode\n# \n# This R code uses dplyr and tibble to create a clean dataset from your PDF text. Here's what the code does:\n# \n# Creates the tibble: Uses tribble() (transpose tibble) to easily input the data in a readable row-by-row format\n# Column names: Uses snake_case naming convention for the education levels\n# Data validation: Includes glimpse() and summary() to check the structure\n# Bonus sorting: Shows how to sort by college education percentage\n# \n# The tibble will have 30 rows (one for each news source) and 4 columns:\n# \n# news_source: Character column with the news outlet names\n# college_plus: Numeric column with percentage having college+ education\n# some_college: Numeric column with percentage having some college\n# high_school_or_less: Numeric column with percentage having high school or less\n# \n# You can run additional dplyr operations on this tibble like filtering, grouping, or creating new calculated columns as needed for your analysis.RetryClaude can make mistakes. Please double-check responses. Sonnet 4\n# \n\n\nThis output simply reads the string and uses the tibble() function to manually declare a data frame from scratch.\n\n\nPrompt 2 Code\n\n\nCode\n# method 2 ----\n\n# Take the same text but use its knowledge of R to write a script that parses the string \n# and creates the same tibble result as the 1st method\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tibble)\n\n# Raw text from PDF\nraw_text &lt;- \"Education levels of U.S. audiences for 30 major news sources Among U.S. adults who regularly get news from ___, % with each level of education College+ Some college High school or less The Atlantic 62% 23% 14% Axios 59 21 19 NPR 58 27 14 The New York Times 56 24 19 The Guardian 53 28 18 The Wall Street Journal 53 26 21 Politico 52 27 20 The Associated Press 52 28 20 BBC News 49 27 23 The Washington Post 47 30 22 HuffPost 47 28 25 The Hill 45 25 29 Forbes 42 30 28 PBS 41 29 30 CNN 38 28 33 New York Post 38 33 29 The Daily Wire 37 37 26 MSNBC 37 30 33 USA Today 34 30 36 Breitbart 34 32 34 Newsweek 32 32 35 NBC News 32 29 39 ABC News 31 30 39 CBS News 30 30 40 Tucker Carlson Network 28 33 40 Newsmax 28 35 37 Fox News 27 30 42 The Joe Rogan Experience 27 35 38 Telemundo 16 26 56 Univision 15 26 57\"\n\n# Parse the text to create tibble\nnews_education &lt;- raw_text %&gt;%\n  # Split into lines and convert to tibble\n  str_split(\"(?=The Atlantic|Axios|NPR|The New York Times|The Guardian|The Wall Street Journal|Politico|The Associated Press|BBC News|The Washington Post|HuffPost|The Hill|Forbes|PBS|CNN|New York Post|The Daily Wire|MSNBC|USA Today|Breitbart|Newsweek|NBC News|ABC News|CBS News|Tucker Carlson Network|Newsmax|Fox News|The Joe Rogan Experience|Telemundo|Univision)\") %&gt;%\n  unlist() %&gt;%\n  # Keep only the lines with actual data (remove header)\n  .[str_detect(., \"The Atlantic|Axios|NPR|The New York Times|The Guardian|The Wall Street Journal|Politico|The Associated Press|BBC News|The Washington Post|HuffPost|The Hill|Forbes|PBS|CNN|New York Post|The Daily Wire|MSNBC|USA Today|Breitbart|Newsweek|NBC News|ABC News|CBS News|Tucker Carlson Network|Newsmax|Fox News|The Joe Rogan Experience|Telemundo|Univision\")] %&gt;%\n  # Convert to tibble\n  tibble(raw_line = .) %&gt;%\n  # Extract news source names and percentages using regex\n  mutate(\n    # Extract news source (everything before the first number)\n    news_source = str_extract(raw_line, \"^[A-Za-z\\\\s]+(?=\\\\s+\\\\d)\"),\n    # Clean up news source names\n    news_source = str_trim(news_source),\n    # Extract all percentages (with or without % sign)\n    percentages = str_extract_all(raw_line, \"\\\\d+%?\")\n  ) %&gt;%\n  # Unnest the percentages and convert to numeric\n  unnest(percentages) %&gt;%\n  # Remove % sign and convert to numeric\n  mutate(\n    percentages = as.numeric(str_remove(percentages, \"%\"))\n  ) %&gt;%\n  # Group by news source and create columns for each education level\n  group_by(news_source) %&gt;%\n  mutate(\n    education_level = case_when(\n      row_number() == 1 ~ \"college_plus\",\n      row_number() == 2 ~ \"some_college\", \n      row_number() == 3 ~ \"high_school_or_less\",\n      TRUE ~ NA_character_\n    )\n  ) %&gt;%\n  # Pivot to wide format\n  pivot_wider(\n    names_from = education_level,\n    values_from = percentages\n  ) %&gt;%\n  # Clean up and select final columns\n  select(news_source, college_plus, some_college, high_school_or_less) %&gt;%\n  ungroup()\n\n# Display the result\nprint(news_education)\n\n\n# A tibble: 30 √ó 4\n   news_source             college_plus some_college high_school_or_less\n   &lt;chr&gt;                          &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n 1 The Atlantic                      62           23                  14\n 2 Axios                             59           21                  19\n 3 NPR                               58           27                  14\n 4 The New York Times                56           24                  19\n 5 The Guardian                      53           28                  18\n 6 The Wall Street Journal           53           26                  21\n 7 Politico                          52           27                  20\n 8 The Associated Press              52           28                  20\n 9 BBC News                          49           27                  23\n10 The Washington Post               47           30                  22\n# ‚Ñπ 20 more rows\n\n\nCode\n# Verify the structure\nglimpse(news_education)\n\n\nRows: 30\nColumns: 4\n$ news_source         &lt;chr&gt; \"The Atlantic\", \"Axios\", \"NPR\", \"The New York Time‚Ä¶\n$ college_plus        &lt;dbl&gt; 62, 59, 58, 56, 53, 53, 52, 52, 49, 47, 47, 45, 42‚Ä¶\n$ some_college        &lt;dbl&gt; 23, 21, 27, 24, 28, 26, 27, 28, 27, 30, 28, 25, 30‚Ä¶\n$ high_school_or_less &lt;dbl&gt; 14, 19, 14, 19, 18, 21, 20, 20, 23, 22, 25, 29, 28‚Ä¶\n\n\nCode\n# Check that percentages add up to ~100% for each source\nnews_education %&gt;%\n  mutate(total = college_plus + some_college + high_school_or_less) %&gt;%\n  select(news_source, total) %&gt;%\n  print()\n\n\n# A tibble: 30 √ó 2\n   news_source             total\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 The Atlantic               99\n 2 Axios                      99\n 3 NPR                        99\n 4 The New York Times         99\n 5 The Guardian               99\n 6 The Wall Street Journal   100\n 7 Politico                   99\n 8 The Associated Press      100\n 9 BBC News                   99\n10 The Washington Post        99\n# ‚Ñπ 20 more rows\n\n\nCode\n# Sort by college+ percentage (descending)\nnews_education_sorted &lt;- news_education %&gt;%\n  arrange(desc(college_plus))\n\nprint(\"Sorted by College+ percentage:\")\n\n\n[1] \"Sorted by College+ percentage:\"\n\n\nCode\nprint(news_education_sorted)\n\n\n# A tibble: 30 √ó 4\n   news_source             college_plus some_college high_school_or_less\n   &lt;chr&gt;                          &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n 1 The Atlantic                      62           23                  14\n 2 Axios                             59           21                  19\n 3 NPR                               58           27                  14\n 4 The New York Times                56           24                  19\n 5 The Guardian                      53           28                  18\n 6 The Wall Street Journal           53           26                  21\n 7 Politico                          52           27                  20\n 8 The Associated Press              52           28                  20\n 9 BBC News                          49           27                  23\n10 The Washington Post               47           30                  22\n# ‚Ñπ 20 more rows\n\n\nThis code goes a little deeper in terms of outlining a whole processing starting with a string and ending up with the same data frame we wrote in Prompt 1. The actual regex (regular expression) code is actually very light; it‚Äôs the processing that follows that requires a bit of grouping, aggregating and pivoting.\n\n\n\nPlotting\nI did ask Claude for some help here and there to get things as close as possible to what Pew published. But I did make some creative decisions and not extend myself too much just to get the justifications and spacing exactly right.\nI‚Äôm not so sure if the names of the colors Pew used are actually vintage merlot and lady flower, but what do you guys think?\n\n\nCode\ndf &lt;- news_education_sorted\nvintage_merlot &lt;- \"#733e48\"\nlady_flower &lt;- \"#cfa5ac\"\n\n\ndf |&gt;\n  ggplot(aes(x = college_plus,\n             y = reorder(news_source, college_plus))) +\n  geom_col(fill = vintage_merlot) +\n  geom_point(aes(x = 36, y = news_source),\n             show.legend = FALSE,\n             color = lady_flower,\n             size = 3) +\n  geom_text(\n    aes(label = college_plus),\n    size = 5,\n    hjust = ifelse(df$college_plus &gt; 34,-.21, 1.3),\n    color = ifelse(df$college_plus &gt; 34, \"black\", \"white\")\n  ) +\n  annotate(\n    \"text\",\n    y = nrow(df) + 2,\n    x = 36,\n    label = \"36% of U.S. adults are\\ncollege graduates.\",\n    lineheight = .8,\n    hjust = 1,\n    vjust = 1,\n    size = 5\n  ) +\n  annotate(\n    \"text\",\n    y = nrow(df) + 3,\n    x = 62,\n    label = \"62% of people who\\nregularly get news\\nfrom The Atlantic are\\ncollege graduates.\",\n    lineheight = .8,\n    hjust = 1,\n    vjust = 1,\n    size = 5\n  ) +\n  labs(title = \"Across U.S. audiences of 30 major news sources, the&lt;br&gt;share with a college degree varies widely\",\n       subtitle = \"Among U.S. adults who regularly get news from __,% who have a &lt;br&gt;bachelor's degree or more education\",\n       caption = \"Source: Pew Research Center survey of 9,482 U.S. adults conducted March 10-16, 2025;&lt;br&gt;the U.S. adult figure is from Pew Research Center analysis of data from the 2024 Current&lt;br&gt;Population Survey, Annual Social and Economic Supplement (IPUMS).\") +\n  scale_y_discrete(expand = expansion(mult = c(0.01, 0.1))) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.5))) +\n  theme_minimal() +\n  theme(\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text = element_markdown(color = \"black\",\n                                family = \"Arial\",\n                                size = 16),\n    plot.title = element_markdown(hjust = 0, \n                                  face = \"bold\",\n                                  size = 22),\n    plot.subtitle = element_markdown(hjust = 0, \n                                 size = 16, \n                                 color = \"grey45\", \n                                 face = \"italic\", \n                                 family = \"Cambria\"),\n    plot.caption = element_markdown(color = \"grey45\",\n                                    size = 13,\n                                    hjust = 0)\n  )\n\n\n\n\n\nI‚Äôm happy as to how this turned out. The next step is to find the raw survey data Pew collected and use that as the starting point rather than the PDF they provided üëÄ"
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Thoughts on Plots",
    "section": "",
    "text": "This is a collection of my plots and their reproducible R scripts.\n\n\n\n\n\n\n\n\n  \n\n\n\n\n25 Years of the Big Mac Index\n\n\n\n\n\nVisualizing global currency valuation through the lens of a hamburger\n\n\n\n\n\n\nNov 18, 2025\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Pew Research‚Äôs ‚ÄòHow the audiences of 30 major news sources differ in their levels of education‚Äô\n\n\n\n\n\nLet‚Äôs ask ClaudeAI to help us with identifying data stored in a string\n\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n  \n\n\n\n\nSources of U.S. Federal Revenue Over The Years Part 2\n\n\n\n\n\nA practice in plot animation\n\n\n\n\n\n\nAug 11, 2025\n\n\n\n\n\n\n  \n\n\n\n\nSources of U.S. Federal Revenue Over The Years Part 1\n\n\n\n\n\nFiguring out how to reproduce a plot found out in the wild\n\n\n\n\n\n\nAug 2, 2025\n\n\n\n\n\n\n  \n\n\n\n\nCalifornia forest fires acres battled by agency\n\n\n\n\n\nAn exercise in effective use of color and axis labels\n\n\n\n\n\n\nJul 19, 2025\n\n\n\n\n\n\n  \n\n\n\n\nMedian Income and Race in the District of Columbia\n\n\n\n\n\nMapping U.S. Census Data with R\n\n\n\n\n\n\nJul 19, 2025\n\n\n\n\n\n\n  \n\n\n\n\nColor Frequency\n\n\n\n\n\nSubmission to Tidy Tuesday for Week 8, 2023\n\n\n\n\n\n\nJul 9, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "demos.html#visualizations",
    "href": "demos.html#visualizations",
    "title": "Thoughts on Plots",
    "section": "Featured Visualizations",
    "text": "Featured Visualizations\n\n\nAdvanced ggplot2 Techniques\n\n# Your existing R visualization code here\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Example plot - replace with your actual demos\n\nKey Features: - Custom themes and styling - Complex data transformations - Interactive elements with plotly\n\n\n\nTime Series Analysis\n\n# Your time series visualization code\n\nDemonstrates: - Trend analysis - Seasonal decomposition - Forecasting methods\n\n\n\nInteractive Dashboards\n\n# Your interactive visualization code\n\nTechnologies: - R Shiny - Plotly - Custom HTML widgets"
  },
  {
    "objectID": "demos.html#case-studies",
    "href": "demos.html#case-studies",
    "title": "Thoughts on Plots",
    "section": "Case Studies",
    "text": "Case Studies\n\nProject: Market Analysis Dashboard\nChallenge: Client needed to understand customer behavior patterns across multiple channels.\nSolution: Built comprehensive analytical framework with interactive visualizations.\nResults: - 40% improvement in insight discovery time - Identification of 3 new market segments - Improved decision-making process\nView Full Case Study ‚Üí\n\n\nProject: Predictive Modeling Pipeline\nChallenge: Automate forecasting process for inventory management.\nApproach: Developed custom R package with modeling and visualization components.\nOutcome: - 25% reduction in forecast error - Streamlined workflow - Reproducible reporting system\nView Technical Details ‚Üí"
  },
  {
    "objectID": "demos.html#additional-examples",
    "href": "demos.html#additional-examples",
    "title": "Thoughts on Plots",
    "section": "Additional Examples",
    "text": "Additional Examples\nBrowse more demonstrations organized by technique and application:\n\n\nStatistical Modeling Examples ‚Üí\nData Cleaning & Transformation ‚Üí\nGeospatial Visualizations ‚Üí\nNetwork Analysis ‚Üí\nText Analytics ‚Üí\n\n\n\n‚Üê Back to Home | View Apps ‚Üí"
  },
  {
    "objectID": "events/coin_des_plots.html",
    "href": "events/coin_des_plots.html",
    "title": "MTA Art Explorer",
    "section": "",
    "text": "Explore the permanent art collection in MTA stations across New York City. This is intended to demonstrate how quickly one can spin up an app that enables quick and easy exploration of a data set from New York State‚Äôs Open Data Portal. The code will be published to GitHub shortly.\nTo identify the cover photo, it is the piece The Habitat for the Yellow Bird by Takayo Noda, and is displayed in the Sutter Ave station on the L line of the NYC Subway.\n\n\nThe data for this app can be found in the MTA‚Äôs Open Data Portal."
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "Plotting & Thotting",
    "section": "",
    "text": "Landing page for all upcoming Solplots events, including ‚ÄúPlotting & Thotting‚Äù series as well as other engagements and sessions.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMTA Art Explorer\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "These are resources I think are useful."
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "Resources",
    "section": "Books",
    "text": "Books\nR for Data Science (2e)\nHadley Wickham, Mine Cetinkaya-Rundel & Garrett Grolemund\nWhenever someone asks me how to get started with learning R programming, I always point them here.\nR Graphics Cookbook, 2nd edition\nWinston Chang\nThis book focuses on R‚Äôs plotting capabilities, primarily through the ggplot2 library. I used this resource when I made the intentional decision to sit down and learn how to code plots in R. It‚Äôs a cookbook meant to cover the possible situations you may encounter when plotting data, so it isn‚Äôt too comprehensive. I didn‚Äôt have really any programming experience before I started reading this book.\nFunctional Programming\nSara Altman, Bill Behrman, Hadley Wickham A practical introduction functional programming in the tidyverse.\nggplot2: Elegant Graphics for Data Analysis\nHadley Wickham\nThis is the comprehensive resource for ggplot2, explained in through detail by Hadley Wickham, the author of several other key R data science package libraries.\nGeocomputation with R\nRobin Lovelace, Jakub Nowosad and Jannes Muenchow\nLearn how to turn R into a GIS by using the simple features (sf) package."
  },
  {
    "objectID": "resources.html#blogs-and-projects",
    "href": "resources.html#blogs-and-projects",
    "title": "Resources",
    "section": "Blogs and Projects",
    "text": "Blogs and Projects\nFlowingData\nNathan Yau, Ph.D\nWell-renown statistician and data visualization artist shares his work (also offers membership that includes tutorials and other learning materials).\nNicola Rennie, a data visualization specialist.\nStatistics Globe\nJoachim Schork\nA comprehensive statistics resource for R and Python.\nR Graph Gallery\nYan Holtz\nThe ‚Äúmost extensive compilation of R-generated graphs‚Äù found on the internet, it contains over 400 examples of over 50 different plot types primarily focusing on R and ggplot2, with some base R plotting as well.\nJulia Silge releases YouTube videos along with each blog post, which are very straightforward and pragmatic analyses\nOutsider Data Science\nArt Steinmetz\nA former Wall Street CEO shares his data stories with R. Lot‚Äôs of different analyses all with reproducible code."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "üìß peter@solplots.com\n\nüíº LinkedIn\n\nüêô GitHub"
  },
  {
    "objectID": "resume.html#data-scientist-i-strategic-routing-worldpay-inc",
    "href": "resume.html#data-scientist-i-strategic-routing-worldpay-inc",
    "title": "Resume",
    "section": "Data Scientist I, Strategic Routing ‚Äî Worldpay, Inc",
    "text": "Data Scientist I, Strategic Routing ‚Äî Worldpay, Inc\nRemote | Dec 2023 ‚Äì Present\n\nImplements network routing product to maximize merchant cost savings on card processing fees\nPerforms ad-hoc analyses simulating potential merchant savings using a ‚Äúleast cost routing‚Äù method\nEngineers routing table update processes by integrating various documents and requests\nMonitors product performance to match negotiated transaction volumes from network deals"
  },
  {
    "objectID": "resume.html#data-scientist-transit-rail-planning-vanasse-hangen-brustlin-inc.-vhb",
    "href": "resume.html#data-scientist-transit-rail-planning-vanasse-hangen-brustlin-inc.-vhb",
    "title": "Resume",
    "section": "Data Scientist, Transit & Rail Planning ‚Äî Vanasse Hangen Brustlin, Inc.¬†(VHB)",
    "text": "Data Scientist, Transit & Rail Planning ‚Äî Vanasse Hangen Brustlin, Inc.¬†(VHB)\nWashington, D.C. | Sept 2022 - Dec 2023\n\nAnalyzed General Transit Feed Specification (GTFS) data in conjunction with Automated Passenger Count data to program R scripts and output load factor and schedule adherence KPI for transit studies\nDeveloped and maintains a suite of transit planning and origin/destination traffic analysis Shiny Apps known as Data Trippers that resulted in assisting clients‚Äô decision making processes"
  },
  {
    "objectID": "resume.html#data-scientist-ohio-kentucky-indiana-regional-council-of-governments-oki",
    "href": "resume.html#data-scientist-ohio-kentucky-indiana-regional-council-of-governments-oki",
    "title": "Resume",
    "section": "Data Scientist ‚Äî Ohio Kentucky Indiana Regional Council of Governments (OKI)",
    "text": "Data Scientist ‚Äî Ohio Kentucky Indiana Regional Council of Governments (OKI)\nCincinnati, Ohio | May 2021 - Aug 2022\n\nRe-purposed open-source R scripts to develop inaugural method evaluating roadway safety performance in the 8-county OKI region which resulted in the Excess Expected Crashes (EEC) KPI\nPresented methodology at area analytics conferences and the useR! Conference 2022 (recognized)\nVisualized of existing conditions and commodity flow using Freight Analysis Framework (FAF)"
  }
]